---
title: "STAT 479 HW3"
author: "Madison Polley"
output: rmarkdown::pdf_document
header-includes:
    - \usepackage {hyperref}
    - \hypersetup {colorlinks = true, linkcolor = red, urlcolor = red}
---

```{r, echo = FALSE, warning=FALSE}
library("knitr")
library(lubridate)
library(dplyr)
library(readr)
library(tsibble)
library(ggplot2)
library(sf)
library(feasts)
library(tsibbledata)
library(tidyr)
library(ggmap)
library(raster)
library(spData)
library(leaflet)
library(spData)
library(RStoolbox)
library(ggraph)
library(tidygraph)
library(igraph)
library(ggnetwork)
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE)
```

## Instructions

1. Submit your solutions to the exercises below before **February March 19 at 11:59pm
CST**.
2. Prepare your solutions as Rmarkdown documents.
3. We give example figures below to guide your work. However, view these only as
suggestions -- we will keep an eye out for improvements over our plots.
4. Include two files in your submission, (a) a pdf of the compiled `.Rmd` file
and (b) the original `.Rmd` file.

## Rubric

2 problems below will be graded according to,

* Correctness: For plots, the displays meet the required specifications. For
conceptual questions, all parts are accurately addressed.
* Attention to detail: Writing is clear and designs are elegant. For example, no
superfluous marks are included, all axes are labeled, text is neither too small
nor too large.
* Code quality (if applicable): Code is concise but readable, properly formatted
and commented.

The remaining problems will be graded for completeness.

## Problems

### Matching Autocorrelation Functions

The purpose of this problem is to build further intuition about
auto-correlation. We'll simulate data with known structure and then see what
happens to the associated autocorrelation functions.

a. The code below simulates a sinusoidal pattern over the course of 2020. Extend
the code so that `date` and `y` are contained in a tsibble object.

```{r}
date <- seq(from = as_date("2020-01-01"), to = as_date("2020-12-31"), by = 1)
x <- seq(0, 12 * 2 * pi, length.out = length(date))
y <- sin(x) + rnorm(length(x), 0, .4)


#Below is my code
sindata = data.frame(x, y, date)
sindata_ts = tsibble(sindata, index = date)

```

b. Using the tsibble object, calculate and visualize the induced autocorrelation
function. Use a maximum lag of 50, and interpret the resulting plot.

```{r}
acf_data = ACF(sindata_ts, y, lag_max = 50)
autoplot(acf_data )
```
**As stated in lecture, gradually decreasing slopes in the ACF suggest trends in the data and possible seasonality.**


c. Write a function to simulate a version of the tsibble above, but with a
linear trend from 0 to `z`, where `z` is an argument to the function.

```{r}
sindata = function(z) {
  date <- seq(from = as_date("2020-01-01"), by = 1, length.out = z)
  x <- seq(0, 12 * 2 * pi, length.out = length(date))
  y <- sin(x) + rnorm(length(x), 0, .4)
return(tsibble(data.frame(x,y,date), index = date))
  }
```


d. Using the function from (c), generate 5 datasets with linear trends of
varying magnitudes. Plot the associated autocorrelation functions and comment on
the relationship between the strength of the trend and the shape of the
function.

```{r}
sin1 = sindata(200)
acf_data1 = ACF(sin1, y, lag_max = 50)
autoplot(acf_data1 )

sin2 = sindata(300)
acf_data2 = ACF(sin2, y, lag_max = 50)
autoplot(acf_data2)

sin3 = sindata(400)
acf_data3 = ACF(sin3, y, lag_max = 50)
autoplot(acf_data3 )

sin4 = sindata(500)
acf_data4 = ACF(sin4, y, lag_max = 50)
autoplot(acf_data4 )

sin5 = sindata(600)
acf_data5 = ACF(sin5, y, lag_max = 50)
autoplot(acf_data5 )
```

**To me, it appears that the seasonaly trends become more and more spaced out as the "z" input increases. There are fewer peaks and toughs as the "z" increases. The strength, however, seems to be the same in each since their peaks and troughs his about the same values in every plot.**


### Spotify Time Series

In this problem, we will study music streaming on Spotify in 2017. We'll start
by looking at some characteristics of the most streamed song, and then will
practice how to extract features from across the collection of most streamed
songs.

a. Let's look at the most streamed song of 2017, which was "Shape of You." The
dataset
[here](https://uwmadison.box.com/shared/static/hvplyr3jy6vbt7s80lqgfx81ai4hdl0q.csv)
contains the number of streams for this song across regions, for each day in
which it was in the Spotify 100 most streamed songs for that region. Create a
`tsibble` object from this dataset, keying by `region` and indexing by `date`.

```{r}
spotify = read.csv("https://uwmadison.box.com/shared/static/hvplyr3jy6vbt7s80lqgfx81ai4hdl0q.csv")
spotify$date = as_date(spotify$date)
spotify_ts = as_tsibble(spotify, key = region, index = date)
```


b. Filter to `region == "global"`, and make a `gg_season` plot by month. Comment
on the what you see.

```{r, fig.height=7, fig.width=7}
gg_season(spotify_ts %>% filter(region == "global"), period = "month", size = 1.5)
```

**It appears that the streams increase in January 2017, stay relatively flat for February 2017, and then decrease for every month after except for a spike in December 2017. The highest streaming month was March 2017. There appears to be seasonality within the months. Possibly weekly periods of seasonality?**

c. Provide a scatterplot showing the relationship between the number of streams
of this song in the US and in Canada. Do the same between the US and Japan.
Briefly comment. **Hint**: Use `pivot_wider` to spread the time series for each
region across columns of a
[reshaped](https://krisrs1128.github.io/stat479/posts/2021-01-27-week4-2/)
dataset.

```{r, fig.height= 10, fig.width=7}
library(gridExtra)
library(scales)

canada = spotify %>% 
  pivot_wider(names_from = region, values_from = streams) %>%
  ggplot( aes(x = us, y = ca)) +
  geom_point(col = "orange") +
  scale_y_continuous( labels = comma) +
  ggtitle("US vs Canada Streams of 'Shape of You'")


japan = spotify %>% 
  pivot_wider(names_from = region, values_from = streams) %>%
  ggplot( aes(x = us, y = jp)) +
  geom_point(col = "hot pink")+
  scale_y_continuous(labels = comma)+
  ggtitle("US vs Japan Streams of 'Shape of You'")

grid.arrange(canada, japan, ncol = 1)

```

**There appears to be a strong, positive, linear trend in the scatter plot of the US and Canada. Note that the streams in Canada go all the way up past 300,000 streams. The relationship between the US streams and Japan streams seems non-linear, negative, and not verys trong. Note that the Japan streams peak around 30,000 streams.**

d. The dataset
[here](https://uwmadison.box.com/shared/static/xj4vupjbicw6c8tbhuynw0pll6yh1w0d.csv)
contains similar data, but for all songs that appeared in the Spotify 100 for at
least 200 days in 2017. We have filtered to only the global totals. Read these
data into a tibble, keyed by `artist:region` and extract features of the
`streams` time series using the `features` function in the feasts library. It is
normal to see a few errors reported by this function, it just means that some of
the statistics could not be calculated.

```{r}
spotifyfull = read.csv("https://uwmadison.box.com/shared/static/xj4vupjbicw6c8tbhuynw0pll6yh1w0d.csv")
spotifyfull$date = as_date(spotifyfull$date)
spotifyfull_ts = as_tsibble(spotifyfull, key = c(artist, track_name, region), index = date)

spotifyfull_features <- spotifyfull_ts %>%
  features(streams, feature_set(pkgs = "feasts"))

```


e. Which tracks had the highest and lowest `trend_strength`'s? Visualize their
streams over the course of the year.

```{r}
#arranging the data by trend strength
sortedtrends = spotifyfull_features %>%
  arrange(trend_strength)

#finding the song names of the highest and lowest trend scores
highesttrend = sortedtrends[length(sortedtrends), ]$track_name
lowesttrend = sortedtrends[1,]$track_name

#Making my plots
hightrend = spotifyfull_ts %>%
  filter(track_name == highesttrend) %>%
  ggplot(aes(x = date, y = streams)) +
  geom_line() +
  ggtitle("Streams Of The Song With The Highest Trend Strength Over Time")

lowtrend = spotifyfull_ts %>%
  filter(track_name == lowesttrend) %>%
  ggplot(aes(x = date, y = streams)) +
  geom_line() +
  ggtitle("Streams Of The Song With The Lowest Trend Strength Over Time")

grid.arrange(hightrend, lowtrend, ncol = 1)
```


**The Track with the highest trend strength was "Safari". The track with the lowest trend strength was "Lose Yourself - Soundtrack Version".**


### NYC Trees

In this problem, we'll use vector data to enrich a visualization of trees in New
York City. In the process, we'll practice reading in and generating summaries of
geospatial data.

a. The data at this
[link](https://uwmadison.box.com/shared/static/t1mk6i4u5ks5bjxaw2c7soe2z8i75m2o.csv)
include a subset of data from the New York City Tree Census. Make a scatterplot
of the locations of all trees in the data, coloring in by tree species group and
faceting by health.

```{r, fig.width=10}


register_google(key = "AIzaSyCey4DCyK-jAdM2x6e1XOrHSeybBjHJoXI") # you can get your own API key

#reading the data
NewYork = read.csv("https://uwmadison.box.com/shared/static/t1mk6i4u5ks5bjxaw2c7soe2z8i75m2o.csv")
NewYork[1:20, ]
NewYork$health = factor(NewYork$health, levels = c("Good", "Fair", "Poor"))

#Finding which coordinates to use
lat = NewYork %>%
  summarise(maxx = max(latitude),
            minn = min(latitude),
            midpt = (maxx + minn) / 2)

long = NewYork %>%
  summarise(maxx = max(longitude),
            minn = min(longitude),
            midpt = (maxx + minn) / 2)

#creating my map
satellite <- get_map(location = c(lon = -73.9848, lat = 40.7232), maptype = "satellite", zoom = 14)

ggmap(satellite) +
  geom_point(data = NewYork, aes(x = longitude, y = latitude, color = species_group), alpha = 0.5) +
  facet_wrap(.~health) +
  theme( axis.text.x = element_text(angle=45, hjust = 1), strip.text = element_text(size=8)) +
  labs(title = "Tree Locations in New York by Tree Health", x = "Longitude", y = "Latitude")
```




b. Suppose we wanted to relate these data to characteristics of the built
environment. We have curated public data on
[roads](https://uwmadison.box.com/shared/static/28y5003s1d0w9nqjnk9xme2n86xazuuj.geojson)
and
[buildings](https://uwmadison.box.com/shared/static/qfmrp9srsoq0a7oj0e7xmgu5spojr33e.geojson)
within the same neighborhood. Read these data into `sf` objects using `read_sf`.
For both datasets, report (i) the associated CRS and (ii) the geometry type
(i.e., one of point, linestring, polygon, multipoint, multilinestring,
multipolygon, geometry collection).

```{r}
#reading my data
roads = read_sf("https://uwmadison.box.com/shared/static/28y5003s1d0w9nqjnk9xme2n86xazuuj.geojson")
buildings = read_sf("https://uwmadison.box.com/shared/static/qfmrp9srsoq0a7oj0e7xmgu5spojr33e.geojson")


ggplot() + geom_sf(data = roads) + labs(title = "Roads Data Mapped")
ggplot(buildings) + geom_sf() + labs(title = "Buildings Data Mapped")
```

**It appears that for the roads data, the CRS is the "miller" one from week 7 lecture 4. The associated geometry type is multilinestring. For the buildings data, the CRS is also the "miller" version. The buildings appear to be multipolygon.**

c. Generate a version of the plot in (a) that has the roads and buildings in the
background. An example result is given in Figure 1.

```{r, fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = roads, color = "black", inherit.aes = F) +
  geom_sf(data = buildings, color = "grey", alpha = 0.5, inherit.aes = F)+
  geom_point(data = NewYork, aes(x = longitude, y = latitude, color = species_group), alpha = 1, size = 1) +
  facet_wrap(.~health) +
  theme( axis.text.x = element_text(angle=45, hjust = 1), strip.text = element_text(size=8)) +
  labs(title = "Tree Locations in New York by Tree Health", x = "Longitude", y = "Latitude")
```


### Himalayan Glaciers

In this problem, we'll apply the reading's discussion of raster data to
understand a
[dataset](https://uwmadison.box.com/shared/static/2z3apyg4t7ct5qd4mcwh9rpr63t02jql.tif)
containing Landsat 7 satellite imagery of a Himalayan glacier system.

a. Read the data into a `brick` object. What is the spatial extent of the file
(that is, within what geographic coordinates do we have data)? How many layers
of sensor measurements are available?

```{r}
satellite = brick("glaciers-small.tif")
satellite
```
**The spatial extend of the file is from 86.51314 to 87.00732 for Longitude, and from 27.63608 to 28.11212 for Latitude. There are 15 layers.**

b. Generate an RGB image of this area. In Landsat 7, the first three layers (B1,
B2, and B3) provide the red, green, and blue channels.

```{r}
#The RGB image of the area
plotRGB(satellite)

#the first 3 layers each with their own plots
sat_df1 <- subset(satellite, 1) %>%
  as.data.frame(xy = TRUE)
ggplot(sat_df1) +
  geom_raster(aes(x = x, y = y, fill = B1)) +
  scale_fill_gradient(low = "white", high = "black") +
  coord_fixed()+
  labs(title = "RGB Image of Layer B1", x = "Longitude", y = "Latitude")

sat_df2 <- subset(satellite, 2) %>%
  as.data.frame(xy = TRUE)
ggplot(sat_df2) +
  geom_raster(aes(x = x, y = y, fill = B2)) +
  scale_fill_gradient(low = "white", high = "black") +
  coord_fixed()+
  labs(title = "RGB Image of Layer B2", x = "Longitude", y = "Latitude")

sat_df3 <- subset(satellite, 3) %>%
  as.data.frame(xy = TRUE)
ggplot(sat_df3) +
  geom_raster(aes(x = x, y = y, fill = B3)) +
  scale_fill_gradient(low = "white", high = "black") +
  coord_fixed() +
  labs(title = "RGB Image of Layer B3", x = "Longitude", y = "Latitude")

```


c. Make a plot of the slopes associated with each pixel within this region. An
example result is shown in Figure 2.

```{r}
satellitedf = subset(satellite, 15 ) %>%
  as.data.frame(xy=T)

ggplot(satellitedf) +
  geom_raster(aes(x = x, y = y, fill = slope)) +
  scale_fill_fermenter(n.breaks = 6, palette = "Spectral")+
  coord_fixed() + 
  labs(title = "Slopes of Pixels Within The Given Region", x = "Longitude", y = "Latitude")


```


### CalFresh Enrollment

In this problem, we will investigate spatial and temporal aspects of enrollment
in CalFresh, a nutritional assistance program in California.

a. The code below reads in the CalFresh data. We've filtered out February 2019,
since benefits were distributed differently in this month, leading to outliers
for most counties. Extract features of the `calfresh` time series using the
`features` function in the feasts library.

```{r}
calfresh <- read_csv("https://uwmadison.box.com/shared/static/rduej9hsc4w3mdethxnx9ccv752f22yr.csv") %>%
  filter(date != "2019 Feb") %>%
  mutate(date = yearmonth(date)) %>%
  as_tsibble(key = county, index = date)

calfresh_features <- calfresh %>%
  features(calfresh, feature_set(pkgs = "feasts"))
head(calfresh_features)
```

b. Visualize CalFresh enrollment over time for the counties with the highest and
lowest `seasonal_strength_year`.

```{r}
#finding the high and low
strengthsort = calfresh_features %>%
  arrange(seasonal_strength_year)
low = strengthsort$county[1]
high = strengthsort$county[length(strengthsort)]

#graphs!

calfresh %>%
  filter(county == low) %>%
  ggplot(aes(x = date, y = calfresh)) +
  geom_line() +
  labs(title = "CalFresh Enrollment Of the County With The Lowest 'seasonal_strength_year'")

calfresh %>%
  filter(county == high) %>%
  ggplot(aes(x = date, y = calfresh)) +
  geom_line() +
  labs(title = "CalFresh Enrollment Of the County With The Highest 'seasonal_strength_year'")

```


c. The code below reads in a vector dataset demarcating the county boundaries in
California. Join in the features dataset from (a) with this these vector data.
Use this to produce a map with each county shaded in by its
`seasonal_strength_year`. An example result is shown in Figure 3.

```{r}
counties <- read_sf("https://uwmadison.box.com/shared/static/gropucqxgqm82yhq13do1ws9k16dnxq7.geojson")

full = counties %>%
  left_join(calfresh_features)
ggplot(full) +
  geom_sf(color = "white", aes(fill = seasonal_strength_year)) +
  scale_fill_viridis_b() +
  labs(title = "Seasonal Strength Year by County in California", x = "Longitude", y = "Latitude")
```

d. Propose, but do not implement, a visualization of this dataset that makes use
of dynamic queries. What questions would the visualization answer? What would be
the structure of interaction, and how would the display update when the user
provides a cue?

**I think a dynamic query where you can both highlight by seasonal_strength_year value and hover over a county to be shown its name and seasonal_strength_year value would be nice. This would show us what areas of California are within a certain range of seasonal_strength_year values and give us a little more information about the counties. We could then easily identify the possible outlier that is pea green above.**


### Political Book Recommendations

In this problem, we'll study a network dataset of Amazon bestselling US Politics
books. Books are linked by an edge if they appeared together in the
recommendations ("customers who bought this book also bought these other
books").

a. The code below reads in the edges and nodes associated with the network. The
edges dataset only contains IDs of co-recommended books, while the nodes data
includes attributes associated with each book. Use the edges dataset to create
an igraph graph object, and use the `ggnetwork` function to construct a
data.frame summarizing the layout of the network. Use a `layout_with_fr` layout,
as in the reading.

```{r}
edges <- read_csv("https://uwmadison.box.com/shared/static/54i59bfc5jhymnn3hsw8fyolujesalut.csv")
nodes <- read_csv("https://uwmadison.box.com/shared/static/u2x392i79jycubo5rhzryxjsvd1jjrdy.csv", col_types = "ccc")

G = graph_from_data_frame(edges, directed = F)
netg = ggnetwork(G, layout = layout_with_fr(G))
head(netg)
```

b. The output from (a) does not include any attributes about the books, since
this is only available in the `nodes` dataset, and we built the graph layout
using only `edges`. Join in the node attribute data. **Hint**: Use `left_join`, but
using the `by` argument to ensure that `name` in the output of `ggnetwork` and
`Id` in the original `nodes` dataset are associated with one another.

```{r}
fulldata = netg %>%
  left_join(nodes, by= c("name" = "Id"))
```


c. Use the result from part (b) to visualize the network using. Include the
book's title in the node label, and shade in the node according to political
ideology. An example result is shown in Figure 4.

```{r, fig.height=10, fig.width=10}
ggplot(fulldata, aes(x=x, y=y, xend = xend, yend=yend))+
  geom_edges(color = "black")+
  geom_nodes(aes(x = x, y = y, color = political_ideology), size = 3, alpha = 0.75) +
  geom_nodelabel(aes(label = Label, fill = political_ideology), size=3) +
  theme_blank() + labs(title = "Book Network Colored by Political Ideology")
```


## Feedback

a. How much time did you spend on this homework?
**About 9 hours**
b. Which problem did you find most valuable?
**I liked graphing California!**
