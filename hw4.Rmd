---
title: "STAT 479 HW4"
author: "Madison Polley"
output: rmarkdown::pdf_document
header-includes:
    - \usepackage {hyperref}
    - \hypersetup {colorlinks = true, linkcolor = red, urlcolor = red}
---

```{r, echo = FALSE}
library("knitr")
library(readr)
library(dplyr)
library(tidyr)
library(superheat)
library(gutenbergr)
library(stringr)
library(tidytext)
library(topicmodels)
library("MASS")
library("tibble")
library("purrr")
library(ggplot2)
library(dslabs)
library(tidymodels)
library(cluster)

library("skimr")
library("tidyverse")
library("naniar")
library("UpSetR")
library("rpart")

library("embed")
library("topicmodels")

opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE)
```

## Instructions

1. Submit your solutions to the exercises below before **April 9 at
11:59pm CST**.
2. Prepare your solutions as an Rmarkdown document.
3. We give example figures below to guide your work. However, view these only as
suggestions -- we will keep an eye out for improvements over our plots.
4. Include two files in your submission, (a) a pdf of the compiled `.Rmd` file
and (b) the original `.Rmd` file.

## Rubric

2 problems below will be graded according to,

* Correctness: For plots, the displays meet the required specifications. For
conceptual questions, all parts are accurately addressed.
* Attention to detail: Writing is clear and designs are elegant. For example, no
superfluous marks are included, all axes are labeled, text is neither too small
nor too large.
* Code quality (if applicable): Code is concise but readable, properly formatted
and commented.

The remaining problems will be graded for completeness.

## Problems

### 1. Polio incidence

In this problem, we will use a heatmap to visualize a large collection of time
series. The
[data](https://uwmadison.box.com/s/nm7yku4y9q7ylvz5kbxya3ouj2njd0x6), prepared
by the [Tycho Project](https://www.tycho.pitt.edu/), contain weekly counts of
new polio cases in the United States, starting as early as 1912 (for some
states).

a. Pivot the raw dataset so that states appear along rows and weeks appear along
columns. Fill weeks that don't have any cases with 0's.

```{r}
polio <- read_csv("https://uwmadison.box.com/shared/static/nm7yku4y9q7ylvz5kbxya3ouj2njd0x6.csv")

poliolong = polio %>%
  pivot_wider(
    names_from = period_start_date,
    values_from = cases,
    values_fill = 0)

poliomat = poliolong %>%
  column_to_rownames(var = "state")
```

b. Use the `superheat` package to make a heatmap of the data from (a). Have the
color of each tile represent `log(1 + cases)`, rather than the raw counts.
Reorder the states using a hierarchical clustering by setting `pretty.order.rows
= TRUE`.

```{r fig.height= 8, fig.width= 12}
superheat(log(1 + poliomat), left.label.text.size = 4, pretty.order.rows = T)
```


c. Supplement the view from part (b) with a barchart showing the US total
incidence during every given week. Interpret the resulting visualization.
*Hint: use the `yt` argument of superheat.*

```{r fig.height= 8, fig.width= 14}
superheat(log(1 + poliomat), left.label.text.size = 4,
          pretty.order.rows = T, yt= colSums(poliomat),
          yt.plot.type = "line", yt.axis.name = "Cases per Period",
          yt.line.col = "black")
```


d. Describe types of annotation would improve the informativeness of the plot
made in part (c). Also, describe one advantage and one disadvantage of
visualizing a collection of time series as a heatmap, instead of as a collection
of line plots.

**I think adding a scale at the bottom that shows some way of measuring the date would definitely be helpful. An advantage of a heatmat is that it really packs a lot of information into one graph. A disadvantage is that most people do not know how to read heat maps.**


### 2. Silhouette statistics simulation

This problem uses simulation to build intuition about silhouette statistics. The
function below simulates a mixture of three Gaussians, with means evenly spaced
out between `(start, 0)` and `(end, 0)`. See Figure 2 for an example simulated
dataset. We will investigate what happens to the silhouette statistics for each
point as the three clusters are made to gradually overlap.

```{r, fig.width = 6, fig.height = 2, fig.cap = "An example simulated dataset from problem 2." }

rm()
mixture_data <- function(start = 0, end = 10, K = 3, n = 100) {
  mu <- cbind(seq(start, end, length.out = K), 0)
  map_dfr(1:K, ~ mvrnorm(n, mu[., ], diag(2)) %>% as_tibble())
}

ggplot(mixture_data()) +
  geom_point(aes(x = V1, y = V2)) +
  coord_fixed() +
  labs(x = "x", y = "y") +
  theme(
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    panel.grid.minor = element_blank()
  )
```

a. Simulate versions of these data where the `end` argument is decreased from 10
to 0.

```{r}
data1 = mixture_data()
data2 = mixture_data(end = 8)
data3 = mixture_data(end = 6)
data4 = mixture_data(end = 4)
data5 = mixture_data(end = 2)
data6 = mixture_data(end = 0)

```


b. Write a function that performs $K$-means and computes the silhouette
statistic given any one of the datasets generated in part (a). Use this function
to compute the silhouette statistics for each point in the simulated datasets.

```{r}
g_kmeans = function(data, K) {
  x = data %>%
    scale()
  
  kmeans(x, center = K) %>%
    augment(data) %>% # creates column ".cluster" with cluster label 
    mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, "sil_width"])
}

kmeans1 = g_kmeans(data1, K=3)
kmeans1$id = 10

kmeans2 = g_kmeans(data1, K=3)
kmeans2$id = 8

kmeans3 = g_kmeans(data1, K=3)
kmeans3$id = 6

kmeans4 = g_kmeans(data1, K=3)
kmeans4$id = 4

kmeans5 = g_kmeans(data1, K=3)
kmeans5$id = 2

kmeans6 = g_kmeans(data1, K=3)
kmeans6$id = 0

kmeansdata = rbind(kmeans1, kmeans2,kmeans3,kmeans4,kmeans5,kmeans6)
```


c. Visualize the silhouette statistics from part (b) overlaid onto the simulated
data. Discuss the results.

```{r}

ggplot(kmeansdata, aes(x = V1, y=V2, col= silhouette)) +
  geom_point() +
  scale_color_viridis_c() +
  facet_wrap(~id, ncol= 3) +
  labs(x = "X", y = "Y") +
  theme(legend.position = "bottom")


```


### 3. Taxi trips

In this problem, we will use hierarchical clustering to find typical taxi trip
trajectories in Porto, Portugal. The data are a subset from the the [ECML / PKDD
2015 Challenge](http://www.geolink.pt/ecmlpkdd2015-challenge/dataset.html) --
the link provides a complete data dictionary. We have preprocessed it into two
formats. The first
([`wide`](https://uwmadison.box.com/shared/static/cv0lij4d9gn3s8m2k98t2ue34oz6sbj5.csv))
includes each taxi trip on its own row, with latitude and longitude coordinates
along the journey given as separate columns (`x_0, y_0` is the origin of the
trip and `x_15`, `y_15` is the destination). The second
([`long`](https://uwmadison.box.com/shared/static/098cjaetm8vy0mufq21mc8i9nue2rr2b.csv))
format spreads each point of the journey into a separate row.

a. Filter the `long` form of the data down to trip `1389986517620000047` and
plot its trajectory as a sequence of points.

```{r}
trips <- read_csv("https://uwmadison.box.com/shared/static/098cjaetm8vy0mufq21mc8i9nue2rr2b.csv",
                  col_types = cols(TRIP_ID = col_character()))
trips_wide <- read_csv("https://uwmadison.box.com/shared/static/cv0lij4d9gn3s8m2k98t2ue34oz6sbj5.csv",
                       col_types = cols(TRIP_ID = col_character()))

trips %>%
  filter(TRIP_ID == "1389986517620000047") %>%
  ggplot(aes(x = x, y=y)) +
  geom_point()  +
  labs(title = "Trajectory for trip 1389986517620000047", x = "Longitude", y = "Latitude")

```

b. We could hierarchically cluster rows in either the `wide` or the `long`
format datasets. How would the interpretation of the results differ between the
two approaches?

**The long dataset clusters seems like it would cluster location data for all locations of all trips. The wide dataset would be clustering data by trip.**

c. Compute a hierarchical clustering of the `wide` format data, using only the
columns starting with `x` or `y` as features.

```{r}
rm(polio, poliolong, poliomat, kmeansdata, data1, data2, data3, data4, data5, data6, trips)
## I was unable to increase my memory or get my code for this problem to run due to memory shortage.
## Kris instructed me to take a smaller sample in order to complete this problem. I tried 18000 and
## that was still too large. I tried 15000 and that was also too large. 12000 was also too large so
## I settled on 10,000. Then my code wouldn't knit so I had to further decrease to 8,000.

trips_small = trips_wide[sample(nrow(trips_wide), 8000), ]

D = trips_small %>%
  column_to_rownames(var = "TRIP_ID") %>%
  dist()

hclust_result = hclust(D)
```


d. Cut the hierarchical clustering tree so that 8 clusters are produced.
Visualize the trajectories of the taxi trips either colored or faceted by their
cluster.

```{r fig.height=8, fig.width=7}

## Once again, this would not run unless I used the smaller data. 
## My clusters look slightly different than the expected graph due to this.

trips <- read_csv("https://uwmadison.box.com/shared/static/098cjaetm8vy0mufq21mc8i9nue2rr2b.csv", col_types = cols(TRIP_ID = col_character()))

x = trips_small %>%
  dplyr::select(- TRIP_ID) %>%
  scale()

trips8 = kmeans(x, center = 8) %>%
  augment(trips_small) %>%
  mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, "sil_width"]) %>%
  dplyr::select("TRIP_ID", ".cluster", "silhouette") %>%
  left_join(trips, by = "TRIP_ID")

ggplot(trips8, aes(x=x, y=y, col= .cluster)) + 
  geom_point(size = 0.65, alpha = 0.1) +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Latitude and Longitude of Trips by Cluster", x="Longitude", y="Latitude") +
  theme(legend.position = "bottom", legend.key.size = unit(1, "cm")) +
  guides(colour = guide_legend(override.aes = list(size=5, alpha = 1)))
```


### 4.Food nutrients

This problem will use PCA to provide a low-dimensional view of a 14-dimensional
nutritional facts
[dataset](https://uwmadison.box.com/shared/static/nmgouzobq5367aex45pnbzgkhm7sur63.csv).
The data were originally curated by the USDA and are regularly used in
[visualization studies](http://bl.ocks.org/syntagmatic/raw/3150059/).

```{r}
nutrients <- read_csv("https://uwmadison.box.com/shared/static/nmgouzobq5367aex45pnbzgkhm7sur63.csv")
```

a. Define a tidymodels `recipe` that normalizes all nutrient features and
specifies that PCA should be performed.

```{r}
nut_recipe <- recipe(~., data = nutrients) %>%
  update_role(id, name, group, group_lumped, new_role = "vals") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nut_prep <- prep(nut_recipe)

component <- tidy(nut_prep, 2)
score <- juice(nut_prep)
variance <- tidy(nut_prep, 2, type = "variance") %>%
  filter(terms == "percent variance")

ggplot(variance) + 
  geom_col(aes(component, value), color = "black", fill = "red") +
  labs(title = "Plot of Normalized Nutrient Features' Component Values")
```

b. Visualize the top 6 principal components. What types of food do you expect to
have low or high values for PC1 or PC2?

```{r fig.height=8, fig.width=10}
component_2 <- component %>%
  filter(component %in% str_c("PC", 1:6)) %>%
  mutate(terms= reorder_within(terms, abs(value), component))

ggplot(component_2, aes(value, terms, fill = component)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ component, scales = "free_y") + 
  labs(y = NULL) +
  theme(axis.text = element_text(size = 10)) +
  scale_y_reordered()
```

**The expected foods with low values for PC1 are Restaurant Foods, Dairy and Egg Products, and Fast Foods. The ones I expect to be high in PC1 are healthier foods with lots of water such as fruits and veggies as well as juices. The foods I expect to be high in PC2 are Fruits and Veggies, maybe Fish and nuts. I think the lowest ones for PC2 will be unhealthy foods such as Fast Foods, Fats and Oils, maybe dairy as well.**

c. Compute the average value of PC2 within each category of the `group` column.
Give the names of the groups sorted by this average.

```{r}
library(forcats)
averages = score %>%
  group_by(group) %>%
  summarise(
    average = mean(PC2)
  ) %>%
  arrange(average)
averages
```


d. Visualize the scores of each food item with respect to the first two
principal components. Facet the visualization according to the `group` column,
and sort the facets according to the results of part (c). How does the result
compare with your guess from part (b)?

```{r fig.height=11, fig.width=10}
library(ggrepel)
score %>%
  mutate(orders = fct_reorder(group, PC2, .fun= mean, .desc=F)) %>%
  ggplot( aes(PC1, PC2, label = name)) +
  geom_point( alpha = 0.7, size = 1.5) +
  facet_wrap(.~orders, nrow = 5) +
  scale_x_continuous(breaks = c(-8, -4, 0)) +
  scale_y_continuous(breaks = c(-5, 0, 5, 10)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme(strip.background = element_blank(), strip.text.x = element_text(size = 8))
  
```

**I think I was close! I didn't expect spices and herbs to be so high in PC1! Bakes products, beverages, and fruits and veggies seem to be highest in PC2. The highest for PC! seem to be spices and herbs, soups sauces and gravies, and fruits and veggie juices.**

### 5. Modeling topics in _Pride and Prejudice_

This problem uses LDA to analyze the full text of _Pride and Prejudice_. The
code below creates two R objects, `paragraph` and `dtm`. `paragraph` is a
data.frame whose rows are paragraphs^[We've filtered very short paragraphs;
e.g., from dialogue.] from the book. `dtm` is a `DocumentTermMatrix` containing
word counts across the same paragraphs -- the $i^{th}$ row of `dtm` corresponds
to the $i^{th}$ row of `paragraph`.

```{r}


paragraphs <- read_csv("https://uwmadison.box.com/shared/static/pz1lz301ufhbedzsj9iioee77r95xz4v.csv") 
dtm <- paragraphs %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word) %>%
  count(paragraph, word) %>%
  cast_dtm(paragraph, word, n)
```

a. Fit an LDA model to `dtm` using 6 topics. Set the seed by using the argument
`control = list(seed = 479)` to remove any randomness in the result.

```{r}
papLDA = LDA(dtm, k=6, control = list(seed=479))
```


b. Visualize the top 30 words within each of the fitted topics. Specifically,
create a faceted bar chart where the heights of the bars correspond to word
probabilities and the facets correspond to topics. Reorder the bars so that each
topic's top words are displayed in order of decreasing probability.

```{r fig.height=10, fig.width=10}
fittedtopics = tidy(papLDA, matrix = "beta")
fittedtopics %>% arrange(topic, -beta)

topwords = fittedtopics %>%
  group_by(topic) %>%
  slice_max(beta, n=30) %>%
  mutate(term = reorder_within(term, beta, topic))

ggplot(topwords, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free") +
  scale_fill_brewer(palette = "Spectral") +
  scale_y_reordered() +
   theme( axis.text.y = element_text( hjust = 1, size=10), strip.text = element_text(size=10))
```


c. Find the paragraph that is the purest representative of Topic 2. That is, if
$\gamma_{ik}$ denotes the weight of topic $k$ in paragraph $i$, then print out
paragraph $i^{\ast}$ where $i^{\ast} = \arg \max_{i}\gamma_{i2}$. Verify that
the at least a few of the words with high probability for this topic appear.
Only copy the first sentence into your solution.

```{r}
memberships = tidy(papLDA, matrix = "gamma")
weights = memberships %>%
  filter(topic == 2) %>%
  arrange(desc(gamma), document, )
# weights[1, ]
# paragraphs[347, ]$text
```

**I found that paragraph 347 had the highest gamma. The first sentence reads: "sir william and lady lucas were speedily applied to for their consent; and it was bestowed with a most joyful alacrity"**

### 6. Single-Cell Genomics

In this problem, we will apply UMAP to a
[dataset](https://uwmadison.box.com/shared/static/ai539s30rjsw5ke4vxbjrxjaiihq7edk.csv)
of Peripheral Blood Mononuclear Cells (PBMC) released by 10X Genomics. The first
column, `cell_tag`, gives an identifier for each cell in the dataset. All other
columns are molecules that were detected in that cell. For example, CD74 is a
molecule often found on the surface of T-cells.

a. Define a tidymodels `recipe` that specifies that UMAP should be performed
with the parameters `learn_rate = 0.1` and `neighbors = 5`. There is no need to
normalize these data, as they have been normalized in advance using methods
tailored to single-cell genomics data.

```{r}
pbmc <- read_csv("https://uwmadison.box.com/shared/static/ai539s30rjsw5ke4vxbjrxjaiihq7edk.csv")


umap_recipe <- recipe(~., data = pbmc) %>%
  update_role(cell_tag, GNLY, new_role = "id") %>%
  step_umap(all_predictors(), neighbors = 5, learn_rate = 0.1)

```

b. Compute the UMAP embeddings across cells. Color points in by their value of
the GNLY molecule.

```{r}
umap_prep <- prep(umap_recipe)

ggplot(juice(umap_prep), aes(umap_1, umap_2)) +
  geom_point(aes(color = GNLY), alpha = 0.5, size = 0.8) +
  scale_color_gradientn(colors = rainbow(5))
```


## Feedback

a. How much time did you spend on this homework?
**About 5 hours**
b. Which problem did you find most valuable?
**Probably the book problem!**